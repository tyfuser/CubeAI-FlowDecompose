"""
è§†é¢‘æ‹æ‘„è¾…åŠ©ç³»ç»Ÿ - ç»†ç²’åº¦åˆ†æ®µåˆ†æè„šæœ¬

æ¯3ç§’åˆ†æä¸€æ¬¡ï¼Œç”Ÿæˆè¯¦ç»†çš„åˆ†æ®µæŠ¥å‘Šã€‚
"""
import json
import sys
import cv2
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import List, Dict

sys.path.insert(0, str(Path(__file__).parent))

from src.models.data_types import BBox, OpticalFlowData, SubjectTrackingData
from src.models.enums import MotionType, SpeedProfile, SuggestedScale


def analyze_segment(frames: List[np.ndarray], start_time: float, end_time: float, fps: float) -> Dict:
    """åˆ†æå•ä¸ªè§†é¢‘ç‰‡æ®µã€‚"""
    if len(frames) < 2:
        return None
    
    # è®¡ç®—å…‰æµ
    flow_magnitudes = []
    flow_angles = []
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    
    for i in range(1, len(frames)):
        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(
            prev_gray, curr_gray, None,
            pyr_scale=0.5, levels=3, winsize=15,
            iterations=3, poly_n=5, poly_sigma=1.2, flags=0
        )
        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        flow_magnitudes.append(np.mean(mag))
        flow_angles.append(np.mean(ang) * 180 / np.pi)
        prev_gray = curr_gray
    
    avg_speed = np.mean(flow_magnitudes) * fps if flow_magnitudes else 0.0
    primary_direction = np.mean(flow_angles) if flow_angles else 0.0
    
    # æ£€æµ‹ä¸»ä½“
    bbox_areas = []
    for frame in frames:
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        thresh = cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11, 2)
        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        if contours:
            largest = max(contours, key=cv2.contourArea)
            x, y, w, h = cv2.boundingRect(largest)
            frame_h, frame_w = frame.shape[:2]
            bbox_areas.append((w / frame_w) * (h / frame_h))
        else:
            bbox_areas.append(0.25)
    
    # è®¡ç®—æŒ‡æ ‡
    frame_pct_change = abs(bbox_areas[-1] - bbox_areas[0]) / bbox_areas[0] if bbox_areas[0] > 0 else 0
    frame_pct_change = min(1.0, frame_pct_change)
    
    variance = np.var(flow_magnitudes) if len(flow_magnitudes) > 1 else 0
    motion_smoothness = 1.0 / (1.0 + variance)
    subject_occupancy = np.mean(bbox_areas)
    
    # æ¨æ–­è¿åŠ¨ç±»å‹
    if avg_speed < 5.0:
        motion_type = "static"
    elif motion_smoothness < 0.4:
        motion_type = "handheld"
    elif frame_pct_change > 0.1:
        motion_type = "dolly_in"
    else:
        direction = primary_direction % 360
        motion_type = "tilt" if 45 <= direction < 135 or 225 <= direction < 315 else "pan"
    
    # é€Ÿåº¦æè¿°
    if frame_pct_change < 0.1:
        speed_desc = "ç¼“æ…¢"
    elif frame_pct_change <= 0.25:
        speed_desc = "ä¸­é€Ÿ"
    else:
        speed_desc = "å¿«é€Ÿ"
    
    # å™¨æå»ºè®®
    if motion_smoothness > 0.7:
        equipment = "æ»‘è½¨/äº‘å°"
    elif motion_smoothness >= 0.4:
        equipment = "æ‰‹æŒäº‘å°"
    else:
        equipment = "é™æ­¢/ä¸‰è„šæ¶"
    
    # ç½®ä¿¡åº¦
    confidence = 0.5 + motion_smoothness * 0.3 + (1 - frame_pct_change) * 0.2
    confidence = min(1.0, max(0.0, confidence))
    
    return {
        "time_range": f"{start_time:.1f}s - {end_time:.1f}s",
        "start_time": start_time,
        "end_time": end_time,
        "avg_motion_px_s": round(avg_speed, 2),
        "frame_pct_change": round(frame_pct_change, 4),
        "motion_smoothness": round(motion_smoothness, 4),
        "subject_occupancy": round(subject_occupancy, 4),
        "motion_type": motion_type,
        "speed_desc": speed_desc,
        "equipment": equipment,
        "confidence": round(confidence, 4),
    }


def analyze_video_segments(video_path: str, segment_duration: float = 3.0) -> List[Dict]:
    """æŒ‰å›ºå®šæ—¶é•¿åˆ†æ®µåˆ†æè§†é¢‘ã€‚"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    frames_per_segment = int(fps * segment_duration)
    
    print(f"è§†é¢‘ä¿¡æ¯: {duration:.2f}ç§’, {fps:.0f}fps, æ¯æ®µ{segment_duration}ç§’({frames_per_segment}å¸§)")
    print(f"é¢„è®¡åˆ†æ®µæ•°: {int(np.ceil(duration / segment_duration))}")
    print("-" * 60)
    
    segments = []
    current_time = 0.0
    segment_idx = 0
    
    while current_time < duration:
        end_time = min(current_time + segment_duration, duration)
        
        # è¯»å–è¯¥æ®µçš„å¸§
        frames = []
        start_frame = int(current_time * fps)
        end_frame = int(end_time * fps)
        
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        
        for _ in range(end_frame - start_frame):
            ret, frame = cap.read()
            if not ret:
                break
            frame_resized = cv2.resize(frame, (640, 360))
            # æ¯éš”å‡ å¸§é‡‡æ ·ä¸€æ¬¡
            if len(frames) < 30:
                frames.append(frame_resized)
        
        if len(frames) >= 2:
            result = analyze_segment(frames, current_time, end_time, fps)
            if result:
                segments.append(result)
                segment_idx += 1
                print(f"[{segment_idx:2d}] {result['time_range']:15s} | {result['motion_type']:10s} | "
                      f"é€Ÿåº¦:{result['speed_desc']:4s} | å¹³æ»‘åº¦:{result['motion_smoothness']:.2%} | "
                      f"ç½®ä¿¡åº¦:{result['confidence']:.2%}")
        
        current_time = end_time
    
    cap.release()
    return segments


def main():
    video_path = "ç”¨æˆ·è§†é¢‘.mp4"
    
    print("=" * 70)
    print("ğŸ¬ è§†é¢‘ç»†ç²’åº¦åˆ†æ®µåˆ†æ (æ¯3ç§’)")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    print(f"\nåˆ†ææ–‡ä»¶: {video_path}\n")
    
    if not Path(video_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    start = datetime.now()
    segments = analyze_video_segments(video_path, segment_duration=3.0)
    elapsed = (datetime.now() - start).total_seconds()
    
    # ç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“Š åˆ†æç»Ÿè®¡")
    print("=" * 70)
    
    motion_types = {}
    for seg in segments:
        mt = seg['motion_type']
        motion_types[mt] = motion_types.get(mt, 0) + 1
    
    print(f"\næ€»åˆ†æ®µæ•°: {len(segments)}")
    print(f"å¤„ç†æ—¶é—´: {elapsed:.2f}ç§’")
    print(f"\nè¿åŠ¨ç±»å‹åˆ†å¸ƒ:")
    for mt, count in sorted(motion_types.items(), key=lambda x: -x[1]):
        print(f"  {mt}: {count}æ®µ ({count/len(segments)*100:.1f}%)")
    
    avg_smoothness = np.mean([s['motion_smoothness'] for s in segments])
    avg_confidence = np.mean([s['confidence'] for s in segments])
    print(f"\nå¹³å‡è¿åŠ¨å¹³æ»‘åº¦: {avg_smoothness:.2%}")
    print(f"å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.2%}")
    
    # ä¿å­˜ç»“æœ
    output = {
        "video": video_path,
        "segment_duration": 3.0,
        "total_segments": len(segments),
        "processing_time": elapsed,
        "statistics": {
            "motion_type_distribution": motion_types,
            "avg_motion_smoothness": round(avg_smoothness, 4),
            "avg_confidence": round(avg_confidence, 4),
        },
        "segments": segments
    }
    
    output_file = f"segment_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜: {output_file}")


if __name__ == "__main__":
    main()

讨论一下phone_ai部分的功能怎么做：
保持现有的功能不退化
你可以这样向你的团队描述这个最终画面：
code Code

    
“想象用户打开了原生相机。

    屏幕最上方有一个半透明的黑色小胶囊，上面写着 AI 的实时指令：‘跟随引导，向右平移’。

    屏幕右边缘有三个白色的 > 符号在微微跳动，暗示往右走。

    屏幕底部有一条细细的平衡线。

    用户开始向右移动手机，底部的平衡线维持在中间的绿色刻度。

    突然用户手抖了一下，平衡线瞬间飙红，上方的文字胶囊也同步闪烁：‘稳住！太快了！’。

    用户慢下来，界面恢复平静，直到结束。”

  

我们让 AI 把“视频里隐含的画面组织逻辑”，

转化为用户在真实场景中可以实践、并在实践后能被清楚解释和复用的能力。

AR 负责在拍摄当下，

帮用户把素材拍“成立”；

脚本负责在拍摄之后，

帮用户理解：

这些素材为什么成立、以后该如何再用。

AI必须在用户打开视频流的时候，通过视频流判断出用户当前所处的环境，然后根据环境和氛围等一系列综合考虑的内容，来发起一个低风险的素材任务

分析拍摄环境中——>输出分析和初步判断

打开视频流后，AI 先做可拍性判读（人/物/空间/混乱兜底），再基于环境与可用性推荐一个低风险素材任务，确保用户在任何场景下都能拍出“锚点成立、关系稳定、信息完成”的可用素材。

AI要判断出当前环境下用户可以表达哪些主题，然后根据这个主题和环境里已有的内容来挑选用户的拍摄东西，在根据他最初敲定的那个脚本，跟用户推荐运镜的手法
：一、 视觉风格定义 (Visual Language)
色板：
主色： 纯白 (#FFFFFF)，带 1px 黑色描边或投影，确保在任何背景下可见。
成功/正常： 系统绿 (System Green) 或 品牌主色（保持克制）。
警告/错误： 警示橙 (Alert Orange) 或 红色。
形态： 扁平化图标，圆角矩形，半透明磨砂玻璃 (Glassmorphism) 背景。
code Code
字体： 系统默认无衬线字体 (San Francisco / Roboto)，字重 Medium。

二、 界面元素重构 (UI Breakdown)
    方向引导 (The Guide) —— 从“赛车箭头”变为“指引标”
我们把巨大的 3D 箭头改成扁平化的 Chevron (V形标) 图标。
code Code

UI 样式：
    不是像路标一样的大箭头，而是屏幕边缘浮动的 >>> 或 ^ 符号。
    平移 (Truck)： 屏幕右边缘出现三个向右的白色箭头 >>>，带呼吸动效。
    推进 (Dolly In)： 屏幕中心出现四个向内收缩的边角（类似相机的对焦框在收缩），或者中心显示一个向上的小箭头 ^。

交互逻辑：
    用户动起来后，箭头透明度降低（表示“正在执行”）。
    如果方向反了，箭头变成红色并闪烁。
    节奏/速度控制 (The Pacing) —— 从“能量条”变为“水平仪/滑块”
不要用那种像“血条”一样的东西，要做成类似曝光补偿滑块或电子水平仪的样式。
code Code

    
UI 样式：
    在屏幕底部（快门上方），放置一条细长的灰色槽。
    中间有一个白色滑块 (Handle)。
    或者是一个简单的半透明横条，上面有波形在动。

交互逻辑（平衡感）：
    完美状态： 滑块稳定在中间绿色区域，或者波形平缓。
    太快/太抖： 滑块冲出安全区变红，或者波形剧烈抖动变成红色杂波。
    这种设计符合“保持平衡”的直觉，而不是“把血条加满”的冲动。

    教练喊话 (The Coach) —— 从“漫画气泡”变为“字幕/通知”
要把 AI 的指导做得像电影字幕或系统通知一样优雅。
code Code

    
UI 样式：
    位置： 屏幕中下方（字幕位）或者 顶部胶囊位（类似灵动岛）。
    样式： 纯文字，无背景或半透明黑色圆角背景。白色文字，字号适中。

交互逻辑：
    Scenario A (正常): 只有淡淡的文字提示：“保持匀速推进...”。
    Scenario B (出错): 文字变红，加粗，这是一种视觉上的“大声说话”。
    Scenario C (完成): 变成绿色打钩图标 + “Nice Shot”。

  

三、 整体画面想象 (Mockup Description)
你可以这样向你的团队描述这个最终画面：
code Code
    
“想象用户打开了原生相机。
    屏幕最上方有一个半透明的黑色小胶囊，上面写着 AI 的实时指令：‘跟随引导，向右平移’。
    屏幕右边缘有三个白色的 > 符号在微微跳动，暗示往右走。
    屏幕底部有一条细细的平衡线。
    用户开始向右移动手机，底部的平衡线维持在中间的绿色刻度。
    突然用户手抖了一下，平衡线瞬间飙红，上方的文字胶囊也同步闪烁：‘稳住！太快了！’。
    用户慢下来，界面恢复平静，直到结束。”


我先快速扫一遍 `phone_ai` 现有能力（API、realtime、agents、前端 controller）和数据流，再把你描述的“原生相机 + AI 实时指令胶囊 + 方向 chevron + 底部水平仪”拆成可落地模块：**环境判读 → 低风险任务生成 → 运镜控制/反馈 → 拍后脚本解释**，保证不破坏现有功能。下面我会先用只读方式定位当前实时分析入口、协议和UI呈现点。

[4 tools called]

Nice，现在能看出 `phone_ai` 已经有一整套“运动分析→平滑度/类型推断→给建议”的基础（你贴的“稳住/太快”其实正好对应它的 `speed/smoothness/hysteresis`）。接下来我会定位：**实时数据是怎么从视频流进来（WebSocket/HTTP）、后端输出什么 advice、前端怎么渲染 overlay**，然后把你想要的三类UI（胶囊提示/边缘chevron/底部水平仪）无缝替换到现有 Advice 展示层里，确保功能不退化。

[6 tools called]

更关键的一点我们已经定位到了：前端 `ShootingAdvisorView` 现在是“摄像头采帧→WebSocket发frames→后端 `RealtimeAnalyzer` 算指标→`AdviceEngine` 出 advice（含 priority/category/message）→前端 `AdviceDisplay` 渲染”。  
所以你要的“胶囊提示/边缘>>>/底部水平仪”本质是：**不改核心分析与阈值体系，重做 advice 的 UI 映射层**；同时再在后端补一层“开流先做环境可拍性判读→低风险任务选择→把任务/运镜目标注入 advice”。下面我继续把 `RealtimeAnalyzer` 的指标、模板词库、以及是否已有 LLM 环境判读钩子读出来。

[16 tools called]

### 先对齐：`phone_ai` 现状 & “不退化”怎么保证
`phone_ai` 现在已经具备一条完整的实时闭环（而且和你描述的“稳住/太快/保持”天然契合）：

- **手机端（Web 原型）**：`story-galaxy-controller/components/CameraCapture.tsx` 采集视频帧 → `ShootingAdvisorView.tsx` 通过 WebSocket 发 `frames`  
- **后端（Python 版）**：`src/realtime/analyzer.py` 做光流/平滑度/方向/主体占比等指标 → `src/realtime/advice_engine.py` 用平滑+滞回输出 `advice`（priority/category/message）→ `src/realtime/websocket_handler.py` 广播给客户端  
- **前端展示层**：`AdviceDisplay.tsx` 按 priority 渲染提示（目前是 banner/角标风格）

所以“不退化”的核心策略是：**保持 `RealtimeAnalyzer` + `AdviceEngine` 的指标、阈值、滞回、cooldown 不动**；我们只在上层增加“任务层”和“HUD 映射层”，并且协议向后兼容（老客户端只看 `advice` 也能跑）。

---

### 你描述的最终画面，其实是 3 个 HUD 元素 + 1 个任务状态机
把“原生相机 + 指令胶囊 + 边缘>>> + 底部水平仪”拆成可实现模块：

- **1) 教练喊话（顶部胶囊 / 字幕位）**  
  - 平时：显示“当前目标任务 + 一句轻提示”（info/positive）  
  - 出错：critical/warning 时**闪烁/变色/加粗**，文案短促（“稳住！太快了！”）
- **2) 方向引导（屏幕边缘 chevrons）**  
  - 目标是“向右平移/向上摇/推进”等时，显示对应方向 `>>>` / `^^`  
  - 用户开始执行后透明度降低；反方向时变红并闪（你想要的“暗示往右走/反了就红”）
- **3) 节奏/速度控制（底部水平仪/滑块）**  
  - 用一个“中心绿色安全区 + 两侧橙/红”表现“平衡感”  
  - 数据来自 `avg_speed_px_frame` + `speed_variance` + `motion_smoothness`（这三个后端已有）
- **4) 任务状态机（开流→判读→任务→执行→纠错→完成）**  
  - `ENV_SCAN`：开流前 1–2 秒做环境可拍性判读  
  - `TASK_PICKED`：给一个低风险任务（锚点成立、关系稳定、信息完成）  
  - `EXECUTING`：HUD 持续引导（方向+节奏+稳定）  
  - `RECOVERY`：抖/快/丢主体时进入纠错（短促红字 + 更强滞回）  
  - `DONE`：给“Nice Shot”并提示下一条素材任务

---

### 数据层怎么做：保留 `advice`，新增“HUD 状态”消息（兼容不退化）
目前手机端只收到 `advice`，但要做“水平仪/方向箭头”的连续动效，**最好让后端同时推一个轻量 telemetry**。

建议协议演进（不破坏现有）：

- **继续保留**：`type: 'advice'`（现在所有功能依赖它）
- **新增（可选）**：`type: 'telemetry'`
  - `avg_speed_px_frame, speed_variance, motion_smoothness, primary_direction_deg, confidence`
- **新增（可选）**：`type: 'task'`
  - `task_id, theme, target_motion ('truck_right'/'dolly_in'/...), target_speed_band, anchor_subject_hint, risk_level`

前端 `ShootingAdvisorView.tsx` 现在对未知 message type 只是 log，所以**加新消息不会让旧 UI 挂掉**；而旧客户端不认识 `telemetry/task` 也不影响 `advice` 的展示——这就是“不退化”的关键。

---

### “开流先判读环境→给低风险素材任务”怎么落地（先规则，后上视觉LLM）
你要的能力本质是：**在任何场景下，都能让用户先拍到“可用素材”**。实现顺序建议：

#### A. 环境可拍性判读（Shootability）
用开流前 1–2 个 buffer（低分辨率）跑一组超快特征（无需大模型也能起效）：
- **光照**：平均亮度、暗部比例（太暗→建议静止+靠近亮源）
- **清晰度/模糊**：拉普拉斯方差（太糊→提醒擦镜头/减少运动）
- **空间可用性**：光流幅度分布 + 画面边缘密度（过于拥挤/杂乱→低风险任务降级）
- **主体线索**：沿用现有“主体检测占位”（现在是边缘密度占格子，未来可接 YOLO）

输出一个结构化结论：
- `environment_tag`（室内/室外/人多/桌面/风景/走廊…）
- `shootability_score`（0–1）
- `constraints`（暗/挤/乱/反光/主体不明显…）
- `recommended_risk_level`（low-only）

#### B. 低风险素材任务库（永远先给能成功的）
做一个小而硬的任务库（不追求花，追求稳）：
- **Anchor Hold（锚点成立）**：静止 2–3 秒锁住主体/空间关系
- **Slow Truck（关系稳定）**：向右/左平移 2–4 秒（最容易做 HUD）
- **Gentle Dolly（信息完成）**：缓慢推进到中景/近景
- **Reveal（低风险揭示）**：先拍遮挡物，再轻移露出主体（空间足够才给）
- **Fallback（混乱兜底）**：静止 + 轻微微调构图（只要能交付“可用镜头”）

选择规则：**先看 constraints 决定能不能动、动到什么程度**；如果 `confidence` 低（你们系统里已有低置信度兜底），就固定给 Anchor Hold。

---

### “主题判断→挑选拍什么→结合脚本推荐运镜”怎么接到现有体系
你们已经有“拍后脚本解释/指令卡”的流水线（`src/services/orchestrator.py` + `agents/*`）。实时这边建议这样衔接：

- **实时阶段（AR/HUD）**：只做“低风险任务 + 运镜控制”，让用户先拍成立
- **拍后阶段（脚本解释/复用）**：把刚才的任务执行结果写回到“指令卡解释层”
  - 例如：这次是 `truck_right`，为什么成立（锚点、关系、信息）+ 下次如何复用（场景条件、速度区间、稳定技巧）

至于“主题判断”，建议先做**轻量主题集合**（如：人物/物件/空间/氛围/行动）并与环境标签组合；后续再把“用户最初敲定脚本（theme/beat/风格）”通过 session 下发到实时任务选择器中（即 `task` 消息）。

---

### UI 视觉语言怎么映射到你们现有组件（最省改动）
你要的视觉风格（白字+描边/投影、玻璃胶囊、克制的绿/橙/红）可以直接替换 `AdviceDisplay.tsx` 的呈现逻辑，而不动它的入参：

- **Coach 胶囊**：把 info/positive/critical/warning 都统一收敛到顶部胶囊（critical 时闪烁+红字）
- **方向 chevrons**：由 `task.target_motion` 或 `telemetry.primary_direction_deg` 决定显示 `>>>`/`^^`（执行中降透明，反向红闪）
- **底部水平仪**：用 `telemetry.avg_speed_px_frame` 映射滑块位置；`speed_variance`/`motion_smoothness` 决定颜色与“飙红”强度

这样做的好处：**不改后端 advice 也能先做出“最终画面感”**；只是在“水平仪连续性”上会更依赖 telemetry（建议尽快加）。

---

### 推荐迭代路线（每一步都不退化）
- **Phase 0（纯 UI）**：只改前端 HUD，把现有 `advice` 文案塞进“胶囊+提示”里，先达成你描述的画面语言
- **Phase 1（telemetry）**：后端每个 buffer 推一次 telemetry，前端水平仪/chevrons 动起来
- **Phase 2（环境判读+低风险任务）**：后端加 `ENV_SCAN` + `TASK_PICKED`，前端显示“跟随引导，向右平移/静止锁定…”
- **Phase 3（主题/脚本联动）**：把用户选的主题/脚本写入 session，任务选择更“像导演”，但依然以 low-risk 为先

---

### 我需要你确认 2 个关键选择
- **你们现在实时跑的是哪个后端？**  
  - `story-galaxy-controller` 的 Node 版（`/shooting/:id/stream`）  
  - 还是 `phone_ai` 的 FastAPI 版（`/api/realtime/session/{id}/ws`）
- **“原生相机”的目标是 Web 原型足够，还是要上 iOS/Android 原生？**  
  这会决定 AR 叠加是继续用网页层（最快）还是接入原生渲染/传感器（更像系统相机）。

你回我这两点，我就能把“任务层字段、telemetry 字段、以及 HUD 映射规则”按你们选定的后端路径整理成一份可以直接开干的接口/模块清单。

我先快速确认 `video_ai_demo` 里当前已经接了哪些模型/提供商（Gemini/OpenAI/Claude）、实时分析链路用的是什么（CV/LLM/混合），然后给你一份“为实现你描述的效果，最小需要哪些模型 + 可选增强模型”的清单，并标注每个模型负责哪一段（环境判读/主题生成/任务生成/话术生成/安全兜底）。

[4 tools called]

我已经确认 `video_ai_demo` 这套后端默认就是“多模态LLM + CV混合”，并且在 README 里明确写了当前主力多模态模型是 **Qwen2.5-VL-7B-Instruct（sophnet）**。接下来我读一下 `app/integrations/mm_llm_client.py` 和配置项，看看它实际怎么调用模型、支不支持换更轻的模型（比如 3B/2B 或者别的 VLM），然后给你一个“必须/可选”的模型调用清单。

[6 tools called]

关键结论是：用 `video_ai_demo` 做你描述的“开流环境判读→给低风险任务→实时 HUD 引导”，**并不需要每帧都调用大模型**。你们已经有实时 CV 指标（光流/平滑度/速度/方向/主体占比）来驱动“水平仪+纠错”；真正需要模型调用的是“开流的语义判读与任务规划”。我再读一下 `video_ai_demo/app/main.py` 和文本 LLM 的配置入口，确认目前可用的模型调用通道（多模态/纯文本）。

[4 tools called]

### 结论：为了实现你描述的“开流判读环境→低风险任务→实时HUD纠错”，**必须调用的是一个多模态视觉语言模型（VLM）**；持续实时反馈部分**不需要模型**（用现有CV指标即可）

### 你在 `video_ai_demo` 里已经具备的“必选模型通道”
- **多模态 VLM（图片输入）**：走 `app/integrations/mm_llm_client.py` 的 `MMHLLMClient`
  - 由环境变量控制：`MM_LLM_BASE_URL` / `MM_LLM_API_KEY` / **`MM_LLM_MODEL`**
  - 项目默认/文档推荐的模型就是：**`Qwen2.5-VL-7B-Instruct`（sophnet OpenAI-compatible）**
  - 用法定位：它现在用于视频拆解，但同一套“发少量帧 + prompt → JSON输出”的机制，正适合你要的 **环境判读/主题猜测/低风险任务生成**（只在开流时调用1次或低频调用）

### 实现目标时“应该调用什么模型”
#### 1) 必须：一个 VLM（做环境与任务）
- **推荐直接用你们已经接通的**：`Qwen2.5-VL-7B-Instruct`
- **调用频率建议**：只在
  - 开流后的前 1–2 秒（取 3–5 张代表帧）做一次“环境可拍性判读 + 低风险任务”
  - 或者检测到“环境明显变化”（例如亮度/主体/场景大变）时再低频复判
- **它负责的输出**（建议让它结构化返回 JSON）：
  - 环境标签：室内/室外/人多/桌面/走廊/逆光/暗光/反光…
  - 可拍性与风险：推荐只给 low-risk 任务
  - 主题候选：人物/物件/空间/氛围/行动
  - 任务建议：拍什么 + 目标运镜（向右平移/推进/静止锁定）+ 目标时长/速度区间 + 兜底方案

> 如果你觉得 7B 延迟或成本偏高：优先把 **`MM_LLM_MODEL` 换成同接口、更轻更快的 VLM**（前提是你所用的服务商提供该模型，比如 Qwen2.5-VL 的更小规格）。代码层不需要大改。

#### 2) 不必须（但很有价值）：一个轻量目标检测模型（做“人/物/混乱兜底”的硬判断）
- 目的：当 VLM 误判/超时/不可用时，系统仍能稳定做“可拍性判读”
- 典型选择：**YOLO 系列的 nano 级**（如 yolov8n 这类），用来快速回答：
  - 是否有人/人脸占比过大/主体是否存在
  - 画面是否过于拥挤（检测框密度）
- 这部分可以本地跑，不依赖云，稳定性会显著提升

#### 3) 可选：纯文本 LLM（只做“更像导演的措辞”）
- 你描述的“稳住！太快了！”完全可以模板化（甚至你们实时 `advice_engine` 已经能做）
- 只有当你想让“教练口吻更自然、结合主题更像人”时，才需要文本 LLM
- 注意：`video_ai_demo` 里虽然有 `app/services/llm_client.py`，但当前 `app/core/config.py` 没看到对应的 `settings.llm` 配置字段；**如果真要用文本 LLM，需要先把配置补齐**，否则这条路可能会直接报错（建议先不依赖它）

### 一句话推荐（最小可行）
- **先用 `Qwen2.5-VL-7B-Instruct` 做开流环境判读+任务生成（低频调用）**  
- **实时HUD纠错用现有 CV（光流/平滑度/速度/方向）**  
- 后续再加一个 **轻量 YOLO** 做兜底，整体会更稳。
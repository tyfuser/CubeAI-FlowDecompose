"""
è§†é¢‘æ‹æ‘„è¾…åŠ©ç³»ç»Ÿ - åŸºäºé•œå¤´åˆ‡æ¢çš„åˆ†å‰²åˆ†æ

é€šè¿‡æ£€æµ‹å¸§é—´å·®å¼‚æ¥è¯†åˆ«é•œå¤´åˆ‡æ¢ç‚¹ï¼Œç„¶åå¯¹æ¯ä¸ªé•œå¤´è¿›è¡Œç‹¬ç«‹åˆ†æã€‚

å¹¶æ˜¾ç¤ºç»™å‡ºä¸­é—´å˜é‡
"""
import json
import sys
import cv2
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Tuple

sys.path.insert(0, str(Path(__file__).parent))

from src.models.enums import MotionType


def detect_shot_boundaries(video_path: str, threshold: float = 30.0) -> List[Tuple[float, float]]:
    """
    æ£€æµ‹é•œå¤´åˆ‡æ¢è¾¹ç•Œã€‚
    
    ä½¿ç”¨å¸§é—´ç›´æ–¹å›¾å·®å¼‚æ¥æ£€æµ‹åœºæ™¯å˜åŒ–ã€‚
    
    Args:
        video_path: è§†é¢‘è·¯å¾„
        threshold: åˆ‡æ¢æ£€æµ‹é˜ˆå€¼ï¼ˆè¶Šå¤§è¶Šä¸æ•æ„Ÿï¼‰
        
    Returns:
        é•œå¤´åˆ—è¡¨ [(start_time, end_time), ...]
    """
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘: {video_path}")
    
    fps = cap.get(cv2.CAP_PROP_FPS)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    duration = total_frames / fps
    
    print(f"è§†é¢‘ä¿¡æ¯: {duration:.2f}ç§’, {fps:.0f}fps, {total_frames}å¸§")
    print(f"é•œå¤´åˆ‡æ¢æ£€æµ‹é˜ˆå€¼: {threshold}")
    print("-" * 60)
    
    # è¯»å–æ‰€æœ‰å¸§çš„ç›´æ–¹å›¾
    prev_hist = None
    frame_diffs = []
    frame_idx = 0
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        # è½¬æ¢ä¸ºHSVå¹¶è®¡ç®—ç›´æ–¹å›¾
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        hist = cv2.calcHist([hsv], [0, 1], None, [50, 60], [0, 180, 0, 256])
        hist = cv2.normalize(hist, hist).flatten()
        
        if prev_hist is not None:
            # è®¡ç®—ç›´æ–¹å›¾å·®å¼‚ï¼ˆä½¿ç”¨ç›¸å…³æ€§ï¼‰
            diff = cv2.compareHist(prev_hist, hist, cv2.HISTCMP_CHISQR)
            frame_diffs.append((frame_idx, diff))
        
        prev_hist = hist
        frame_idx += 1
    
    cap.release()
    
    # æ‰¾åˆ°åˆ‡æ¢ç‚¹ï¼ˆå·®å¼‚è¶…è¿‡é˜ˆå€¼çš„å¸§ï¼‰
    cut_frames = [0]  # å¼€å§‹å¸§
    
    for idx, diff in frame_diffs:
        if diff > threshold:
            # é¿å…è¿ç»­æ£€æµ‹
            if idx - cut_frames[-1] > fps * 0.5:  # è‡³å°‘é—´éš”0.5ç§’
                cut_frames.append(idx)
    
    cut_frames.append(total_frames)  # ç»“æŸå¸§
    
    # è½¬æ¢ä¸ºæ—¶é—´æ®µ
    shots = []
    for i in range(len(cut_frames) - 1):
        start_time = cut_frames[i] / fps
        end_time = cut_frames[i + 1] / fps
        if end_time - start_time >= 0.5:  # è‡³å°‘0.5ç§’çš„é•œå¤´
            shots.append((start_time, end_time))
    
    return shots


def analyze_shot(video_path: str, start_time: float, end_time: float, shot_idx: int, verbose: bool = True) -> Dict:
    """
    åˆ†æå•ä¸ªé•œå¤´ï¼Œæ˜¾ç¤ºæ‰€æœ‰ä¸­é—´è®¡ç®—è¿‡ç¨‹ã€‚
    
    Args:
        video_path: è§†é¢‘è·¯å¾„
        start_time: å¼€å§‹æ—¶é—´
        end_time: ç»“æŸæ—¶é—´
        shot_idx: é•œå¤´åºå·
        verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¸­é—´è®¡ç®—
    """
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    start_frame = int(start_time * fps)
    end_frame = int(end_time * fps)
    total_shot_frames = end_frame - start_frame
    
    cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
    
    frames = []
    for _ in range(min(total_shot_frames, 90)):  # æœ€å¤š90å¸§
        ret, frame = cap.read()
        if not ret:
            break
        frame_resized = cv2.resize(frame, (640, 360))
        if len(frames) < 30:  # é‡‡æ ·30å¸§
            frames.append(frame_resized)
    
    cap.release()
    
    if len(frames) < 2:
        return None
    
    if verbose:
        print(f"\n{'='*70}")
        print(f"ğŸ“¹ é•œå¤´ {shot_idx} è¯¦ç»†åˆ†æ")
        print(f"{'='*70}")
        print(f"æ—¶é—´èŒƒå›´: {start_time:.2f}s - {end_time:.2f}s (æ—¶é•¿: {end_time-start_time:.2f}s)")
        print(f"å¸§èŒƒå›´: {start_frame} - {end_frame} (å…±{total_shot_frames}å¸§)")
        print(f"é‡‡æ ·å¸§æ•°: {len(frames)}å¸§")
    
    # ========== 1. è®¡ç®—å…‰æµ ==========
    flow_magnitudes = []
    flow_angles = []
    flow_x_components = []
    flow_y_components = []
    
    prev_gray = cv2.cvtColor(frames[0], cv2.COLOR_BGR2GRAY)
    
    if verbose:
        print(f"\n--- å…‰æµè®¡ç®— (Farnebackç®—æ³•) ---")
        print(f"å‚æ•°: pyr_scale=0.5, levels=3, winsize=15, iterations=3")
    
    for i in range(1, len(frames)):
        curr_gray = cv2.cvtColor(frames[i], cv2.COLOR_BGR2GRAY)
        flow = cv2.calcOpticalFlowFarneback(
            prev_gray, curr_gray, None,
            pyr_scale=0.5, levels=3, winsize=15,
            iterations=3, poly_n=5, poly_sigma=1.2, flags=0
        )
        
        # è®¡ç®—å¹…åº¦å’Œè§’åº¦
        mag, ang = cv2.cartToPolar(flow[..., 0], flow[..., 1])
        
        avg_mag = float(np.mean(mag))
        avg_ang = float(np.mean(ang) * 180 / np.pi)
        avg_flow_x = float(np.mean(flow[..., 0]))
        avg_flow_y = float(np.mean(flow[..., 1]))
        
        flow_magnitudes.append(avg_mag)
        flow_angles.append(avg_ang)
        flow_x_components.append(avg_flow_x)
        flow_y_components.append(avg_flow_y)
        
        prev_gray = curr_gray
    
    # ========== 2. å…‰æµç»Ÿè®¡ ==========
    avg_magnitude = float(np.mean(flow_magnitudes)) if flow_magnitudes else 0.0
    std_magnitude = float(np.std(flow_magnitudes)) if flow_magnitudes else 0.0
    min_magnitude = float(np.min(flow_magnitudes)) if flow_magnitudes else 0.0
    max_magnitude = float(np.max(flow_magnitudes)) if flow_magnitudes else 0.0
    variance = float(np.var(flow_magnitudes)) if len(flow_magnitudes) > 1 else 0.0
    
    avg_angle = float(np.mean(flow_angles)) if flow_angles else 0.0
    avg_flow_x = float(np.mean(flow_x_components)) if flow_x_components else 0.0
    avg_flow_y = float(np.mean(flow_y_components)) if flow_y_components else 0.0
    
    # è½¬æ¢ä¸ºåƒç´ /ç§’
    avg_speed = avg_magnitude * fps
    
    if verbose:
        print(f"\nå…‰æµå¹…åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å¹…åº¦: {avg_magnitude:.4f} px/å¸§")
        print(f"  æ ‡å‡†å·®: {std_magnitude:.4f}")
        print(f"  æœ€å°å€¼: {min_magnitude:.4f}")
        print(f"  æœ€å¤§å€¼: {max_magnitude:.4f}")
        print(f"  æ–¹å·®: {variance:.4f}")
        print(f"  å¹³å‡é€Ÿåº¦: {avg_speed:.2f} px/s (= {avg_magnitude:.4f} Ã— {fps:.0f}fps)")
        
        print(f"\nå…‰æµæ–¹å‘ç»Ÿè®¡:")
        print(f"  å¹³å‡è§’åº¦: {avg_angle:.2f}Â°")
        print(f"  å¹³å‡Xåˆ†é‡: {avg_flow_x:.4f} (æ­£=å‘å³)")
        print(f"  å¹³å‡Yåˆ†é‡: {avg_flow_y:.4f} (æ­£=å‘ä¸‹)")
        
        # æ˜¾ç¤ºå‰5å¸§çš„å…‰æµå€¼
        print(f"\né€å¸§å…‰æµå¹…åº¦ (å‰5å¸§):")
        for i, mag in enumerate(flow_magnitudes[:5]):
            print(f"  å¸§{i+1}: {mag:.4f} px/å¸§")
        if len(flow_magnitudes) > 5:
            print(f"  ... (å…±{len(flow_magnitudes)}å¸§)")
    
    # ========== 3. è®¡ç®—è¿åŠ¨å¹³æ»‘åº¦ ==========
    motion_smoothness = 1.0 / (1.0 + variance)
    
    if verbose:
        print(f"\n--- è¿åŠ¨å¹³æ»‘åº¦è®¡ç®— ---")
        print(f"å…¬å¼: smoothness = 1 / (1 + variance)")
        print(f"è®¡ç®—: smoothness = 1 / (1 + {variance:.4f}) = {motion_smoothness:.4f}")
        print(f"è§£é‡Š: æ–¹å·®è¶Šå°ï¼Œå¹³æ»‘åº¦è¶Šé«˜")
    
    # ========== 4. æ¨æ–­è¿åŠ¨ç±»å‹ ==========
    if verbose:
        print(f"\n--- è¿åŠ¨ç±»å‹æ¨æ–­ ---")
        print(f"åˆ¤æ–­æ¡ä»¶:")
        print(f"  avg_speed < 5.0 â†’ static")
        print(f"  smoothness < 0.4 â†’ handheld")
        print(f"  smoothness > 0.8 â†’ dolly/track")
        print(f"  å…¶ä»– â†’ pan/tilt")
    
    if avg_speed < 5.0:
        motion_type = "static"
        type_reason = f"avg_speed({avg_speed:.2f}) < 5.0"
    elif motion_smoothness < 0.4:
        motion_type = "handheld"
        type_reason = f"smoothness({motion_smoothness:.4f}) < 0.4"
    elif motion_smoothness > 0.8:
        motion_type = "dolly/track"
        type_reason = f"smoothness({motion_smoothness:.4f}) > 0.8"
    else:
        motion_type = "pan/tilt"
        type_reason = f"0.4 <= smoothness({motion_smoothness:.4f}) <= 0.8"
    
    if verbose:
        print(f"ç»“æœ: {motion_type} (å› ä¸º {type_reason})")
    
    # ========== 5. æ‹æ‘„è´¨é‡è¯„ä¼° ==========
    if verbose:
        print(f"\n--- æ‹æ‘„è´¨é‡è¯„ä¼° ---")
        print(f"è¯„ä¼°æ ‡å‡† (åŸºäºè¿åŠ¨å¹³æ»‘åº¦):")
        print(f"  A (ä¼˜ç§€): smoothness > 0.8")
        print(f"  B (è‰¯å¥½): smoothness > 0.5")
        print(f"  C (ä¸€èˆ¬): smoothness > 0.3")
        print(f"  D (éœ€æ”¹è¿›): smoothness <= 0.3")
    
    if motion_smoothness > 0.8:
        quality = "ä¼˜ç§€"
        quality_score = "A"
    elif motion_smoothness > 0.5:
        quality = "è‰¯å¥½"
        quality_score = "B"
    elif motion_smoothness > 0.3:
        quality = "ä¸€èˆ¬"
        quality_score = "C"
    else:
        quality = "éœ€æ”¹è¿›"
        quality_score = "D"
    
    if verbose:
        print(f"ç»“æœ: {quality_score} ({quality})")
    
    return {
        "start_time": round(start_time, 2),
        "end_time": round(end_time, 2),
        "duration": round(end_time - start_time, 2),
        "frame_range": [start_frame, end_frame],
        "sampled_frames": len(frames),
        # å…‰æµåŸå§‹æ•°æ®
        "optical_flow": {
            "avg_magnitude_per_frame": round(avg_magnitude, 4),
            "std_magnitude": round(std_magnitude, 4),
            "min_magnitude": round(min_magnitude, 4),
            "max_magnitude": round(max_magnitude, 4),
            "variance": round(variance, 4),
            "avg_angle_deg": round(avg_angle, 2),
            "avg_flow_x": round(avg_flow_x, 4),
            "avg_flow_y": round(avg_flow_y, 4),
        },
        # è®¡ç®—ç»“æœ
        "avg_motion_px_s": round(avg_speed, 2),
        "motion_smoothness": round(motion_smoothness, 4),
        "motion_type": motion_type,
        "motion_type_reason": type_reason,
        "quality": quality,
        "quality_score": quality_score,
    }


def main():
    video_path = "ç”¨æˆ·è§†é¢‘.mp4"
    
    print("=" * 70)
    print("ğŸ¬ åŸºäºé•œå¤´åˆ‡æ¢çš„è§†é¢‘åˆ†å‰²åˆ†æ")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    print(f"\nåˆ†ææ–‡ä»¶: {video_path}\n")
    
    if not Path(video_path).exists():
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        return
    
    # æ£€æµ‹é•œå¤´åˆ‡æ¢
    print("ğŸ” æ£€æµ‹é•œå¤´åˆ‡æ¢ç‚¹...")
    shots = detect_shot_boundaries(video_path, threshold=25.0)
    print(f"\næ£€æµ‹åˆ° {len(shots)} ä¸ªé•œå¤´\n")
    
    # åˆ†ææ¯ä¸ªé•œå¤´
    print("\nğŸ“Š åˆ†æå„é•œå¤´...")
    
    results = []
    for i, (start, end) in enumerate(shots, 1):
        result = analyze_shot(video_path, start, end, shot_idx=i, verbose=True)
        if result:
            results.append(result)
    
    # ç»Ÿè®¡
    print("\n" + "=" * 70)
    print("ğŸ“‹ æ±‡æ€»è¡¨æ ¼")
    print("=" * 70)
    print(f"\n{'é•œå¤´':<6} {'æ—¶é—´èŒƒå›´':<18} {'æ—¶é•¿':<8} {'è¿åŠ¨ç±»å‹':<12} {'å¹³æ»‘åº¦':<10} {'è´¨é‡':<6}")
    print("-" * 70)
    for i, r in enumerate(results, 1):
        print(f"{i:<6} {r['start_time']:.1f}s - {r['end_time']:.1f}s{'':<4} {r['duration']:.1f}s{'':<4} "
              f"{r['motion_type']:<12} {r['motion_smoothness']:.2%}{'':<4} {r['quality_score']}")
    
    print("\n" + "=" * 70)
    print("ğŸ“‹ åˆ†ææ‘˜è¦")
    print("=" * 70)
    
    print(f"\næ€»é•œå¤´æ•°: {len(results)}")
    
    # è¿åŠ¨ç±»å‹åˆ†å¸ƒ
    motion_types = {}
    for r in results:
        mt = r['motion_type']
        motion_types[mt] = motion_types.get(mt, 0) + 1
    
    print("\nè¿åŠ¨ç±»å‹åˆ†å¸ƒ:")
    for mt, count in sorted(motion_types.items(), key=lambda x: -x[1]):
        print(f"  {mt}: {count}ä¸ªé•œå¤´")
    
    # è´¨é‡åˆ†å¸ƒ
    quality_dist = {}
    for r in results:
        q = r['quality_score']
        quality_dist[q] = quality_dist.get(q, 0) + 1
    
    print("\næ‹æ‘„è´¨é‡åˆ†å¸ƒ:")
    for q in ['A', 'B', 'C', 'D']:
        count = quality_dist.get(q, 0)
        if count > 0:
            print(f"  {q} ({['ä¼˜ç§€','è‰¯å¥½','ä¸€èˆ¬','éœ€æ”¹è¿›'][['A','B','C','D'].index(q)]}): {count}ä¸ªé•œå¤´")
    
    avg_smoothness = np.mean([r['motion_smoothness'] for r in results])
    print(f"\nå¹³å‡è¿åŠ¨å¹³æ»‘åº¦: {avg_smoothness:.2%}")
    
    # ä¿å­˜ç»“æœ
    output = {
        "video": video_path,
        "total_shots": len(results),
        "motion_type_distribution": motion_types,
        "quality_distribution": quality_dist,
        "avg_smoothness": float(avg_smoothness),
        "shots": results
    }
    
    output_file = f"shot_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ“„ ç»“æœå·²ä¿å­˜: {output_file}")


if __name__ == "__main__":
    main()
